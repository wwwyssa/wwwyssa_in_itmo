# Алгебра. Глава 8. Линейные отображения

# Линейные отображения. Ядро и образ линейного отображения.
## Линейные отображения

- Далее везде $U$ и $V$ — линейные пространства над одним и тем же полем $K$.

### Определение

1. Отображение $\varphi : U \to V$ называется линейным отображением, если оно является гомоморфизмом линейных пространств, то есть,
   $$
   \varphi(\alpha x + \beta y) = \alpha \varphi(x) + \beta \varphi(y)
   $$
   для всех $\alpha, \beta \in K$ и $x, y \in U$.
2. Ядро линейного отображения $\varphi$ — это
   $$
   \ker(\varphi) = \{x \in U : \varphi(x) = 0\}.
   $$
3. Образ линейного отображения $\varphi$ — это
   $$
   \text{Im}(\varphi) = \{y \in V : \exists x \in U \ \varphi(x) = y\}.
   $$

### Лемма 1

- Пусть $\varphi : U \to V$ — линейное отображение. Тогда:
  1. $\ker(\varphi)$ — линейное подпространство $U$;
  2. $\text{Im}(\varphi)$ — линейное подпространство $V$.

#### Доказательство

1. **$\ker(\varphi)$ — линейное подпространство $U$:**
   - Достаточно доказать, что $\ker(\varphi)$ замкнуто по взятию линейной комбинации.
   - Пусть $x, x' \in \ker(\varphi)$ и $\alpha, \beta \in K$.
   - Тогда
     $$
     \varphi(\alpha x + \beta x') = \alpha \varphi(x) + \beta \varphi(x') = 0,
     $$
     а значит, $\alpha x + \beta x' \in \ker(\varphi)$.

2. **$\text{Im}(\varphi)$ — линейное подпространство $V$:**
   - Достаточно доказать, что $\text{Im}(\varphi)$ замкнуто по взятию линейной комбинации.
   - Пусть $y, y' \in \text{Im}(\varphi)$ и $\alpha, \beta \in K$.
   - Тогда существуют такие $x, x' \in U$, что $y = \varphi(x)$ и $y' = \varphi(x')$.
   - Следовательно,
     $$
     \varphi(\alpha x + \beta x') = \alpha \varphi(x) + \beta \varphi(x') = \alpha y + \beta y',
     $$
     а значит, $\alpha y + \beta y' \in \text{Im}(\varphi)$.

# Соответствие линейных отображений и матриц.

## Соответствие линейных отображений и матриц

- Пусть $U$ и $V$ — линейные пространства над полем $K$, в которых зафиксированы базисы $u_1, \ldots, u_m$ и $v_1, \ldots, v_n$ соответственно.
- Тогда любой $x \in U$ представляется как столбец:
  $$
  x = x_1u_1 + \cdots + x_mu_m = \begin{pmatrix} x_1 \\ \vdots \\ x_m \end{pmatrix}
  $$
  (столбец координат в разложении по базису, здесь $x_1, \ldots, x_m \in K$).
- Аналогично, $y \in V$ представляется в виде:
  $$
  y = y_1v_1 + \cdots + y_nv_n = \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}.
  $$
- Пусть $\varphi : U \to V$ — линейное отображение.
- Подставим в $\varphi$ базисные вектора пространства $U$ и разложим результаты по базису $V$:
  $$
  \varphi(u_1) = \begin{pmatrix} a_{1,1} \\ \vdots \\ a_{n,1} \end{pmatrix} = A^{(1)}, \quad \ldots, \quad \varphi(u_i) = \begin{pmatrix} a_{1,i} \\ \vdots \\ a_{n,i} \end{pmatrix} = A^{(i)}, \quad \ldots, \quad \varphi(u_m) = \begin{pmatrix} a_{1,m} \\ \vdots \\ a_{n,m} \end{pmatrix} = A^{(m)}.
  $$
- Тогда $A = \begin{pmatrix} A^{(1)} & \cdots & A^{(m)} \end{pmatrix} \in M_{n,m}(K)$ (матрица с указанными столбцами) — матрица отображения $\varphi$ в фиксированных нами базисах.
- Смысл этой матрицы в том, что если:
  $$
  x = \begin{pmatrix} x_1 \\ \vdots \\ x_m \end{pmatrix} \in U \quad \text{(координаты в базисе $u_1, \ldots, u_m$)},
  $$
  то:
  $$
  \varphi(x) = A \cdot \begin{pmatrix} x_1 \\ \vdots \\ x_m \end{pmatrix} = \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} \quad \text{(координаты $\varphi(x)$ в базисе $v_1, \ldots, v_n$)}.
  $$
- Проверим это:
  $$
  \varphi(x) = \varphi(x_1u_1 + \cdots + x_mu_m) = x_1\varphi(u_1) + \cdots + x_m\varphi(u_m) = A^{(1)}x_1 + \cdots + A^{(m)}x_m = A \cdot \begin{pmatrix} x_1 \\ \vdots \\ x_m \end{pmatrix}.
  $$
- Таким образом, при фиксированных базисах линейное отображение — это просто умножение на матрицу.
- Как правило, оно записывается в виде $Ax = y$, где вектора $x \in U$ и $y \in V$ — столбцы коэффициентов в разложении по фиксированным нами базисам.
- Наоборот, представим вектора из $U$ и $V$ как столбцы координат, пусть $A \in M_{m,n}(K)$.
- Несложно понять, что отображение $\varphi : U \to V$, заданное формулой $\varphi(x) = Ax$, является линейным и в данных базисах будет иметь как раз матрицу $A$.
- Линейность проверяется тривиально, а, подставив $x = u_i$ — столбец из нулей и одной единицы на $i$ месте, мы получим, что $i$-й столбец матрицы отображения должен быть как раз $A^{(i)}$.
- Таким образом, при фиксации базисов $U$ и $V$ существует биекция между линейными отображениями из $U$ в $V$ и матрицами из $M_{m,n}(K)$ (каждому отображению соответствует его матрица).

# Композиция линейных отображений и умножение матриц
## Композиция линейных отображений. Связь с умножением матриц

### Теорема 1
Пусть $U$, $V$, $W$ — конечномерные линейные пространства над полем $K$, а $\varphi : U \to V$ и $\psi : V \to W$ — линейные отображения, имеющие в фиксированных базисах пространств матрицы $A_\varphi$ и $A_\psi$ соответственно. Тогда композиция $\psi \cdot \varphi$ имеет в этих же базисах матрицу $A_\psi \cdot A_\varphi$.

- Пусть $\dim(U) = m$, $\dim(V) = n$, $\dim(W) = \ell$. Тогда:
  $$
  A_\psi \in M_{\ell,n}(K) \quad \text{и} \quad A_\varphi \in M_{n,m}(K),
  $$
  то есть, произведение матриц $A_\psi \cdot A_\varphi$ определено корректно.

### Доказательство Теоремы 1
- Все вектора в нашем доказательстве записаны как столбцы координат в соответствующем фиксированном базисе.
- Тогда для $x \in U$ имеем:
  $$
  (\psi \cdot \varphi)(x) = \psi(\varphi(x)) = \psi(A_\varphi x) = (A_\psi \cdot A_\varphi)x,
  $$
  откуда следует утверждение теоремы.

# Сумма размерностей ядра и образа линейного отображения.


## Теорема 2

Пусть $\varphi : U \to V$ — линейное отображение, $U$ — конечномерное линейное пространство. Тогда:
$$
\dim(\ker(\varphi)) + \dim(\text{Im}(\varphi)) = \dim(U).
$$

### Доказательство

- Пусть $e_1, \ldots, e_k$ — базис $\ker(\varphi)$.
- Дополним его до базиса $U$: $e_1, \ldots, e_k, u_1, \ldots, u_m$.
- Тогда любой вектор $x \in U$ единственным образом представляется в виде:
  $$
  x = \alpha_1 e_1 + \cdots + \alpha_k e_k + \beta_1 u_1 + \cdots + \beta_m u_m,
  $$
  а значит:
  $$
  \varphi(x) = \sum_{i=1}^k \alpha_i \varphi(e_i) + \sum_{i=1}^m \beta_i \varphi(u_i) = \sum_{i=1}^m \beta_i \varphi(u_i).
  $$
- Таким образом, $\varphi(u_1), \ldots, \varphi(u_m)$ — порождающая система векторов в $\text{Im}(\varphi)$.

- Докажем, что вектора $\varphi(u_1), \ldots, \varphi(u_m)$ линейно независимы.
  - Пусть:
    $$
    \sum_{i=1}^m \beta_i \varphi(u_i) = 0.
    $$
  - Пусть $x = \sum_{i=1}^m \beta_i u_i$. Тогда:
    $$
    \varphi(x) = \sum_{i=1}^m \beta_i \varphi(u_i) = 0.
    $$
  - Следовательно, $x \in \ker(\varphi)$, а значит, вектор $x$ можно разложить по базису $\ker(\varphi)$: пусть:
    $$
    x = \sum_{i=1}^k \alpha_i e_i.
    $$
  - Таким образом, мы имеем два разложения вектора $x$ по базису $e_1, \ldots, e_k, u_1, \ldots, u_m$ пространства $U$:
    $$
    x = \sum_{i=1}^k \alpha_i e_i = \sum_{i=1}^m \beta_i u_i.
    $$
  - Но эти два разложения должны совпадать! Следовательно:
    $$
    \alpha_1 = \cdots = \alpha_k = \beta_1 = \cdots = \beta_m = 0.
    $$
  - Таким образом, вектора $\varphi(u_1), \ldots, \varphi(u_m)$ линейно независимы, то есть, это базис $\text{Im}(\varphi)$.

- Следовательно:
  $$
  \dim(\ker(\varphi)) + \dim(\text{Im}(\varphi)) = k + m = \dim(U).
  $$

# Размерности ядра и образа линейного отображения: связь с рангом матрицы отображения.
## Теорема 3

Пусть $U$, $V$ — конечномерные линейные пространства над полем $K$, в которых фиксированы базисы, а $\varphi : U \to V$ — линейное отображение с матрицей $A$. Тогда:
$$
\dim(\text{Im}(\varphi)) = \text{rk}(A).
$$

### Доказательство

- Как обычно, пусть $\dim(U) = m$, а $u_1, \ldots, u_m$ — базис $U$. Тогда:
  $$
  \text{rk}(A) = \dim(\text{Lin}(A^{(1)}, \ldots, A^{(m)})) = \dim(\text{Lin}(\varphi(u_1), \ldots, \varphi(u_m))).
  $$
- А теперь заметим, что:
  $$
  \text{Lin}(\varphi(u_1), \ldots, \varphi(u_m)) = \{\sum_{i=1}^m\alpha_i\varphi(u_i)\ : \ \alpha_1,...,\alpha_m \in K \} = \\ \{\varphi\left(\sum_{i=1}^m \alpha_i u_i\right) : \alpha_1, \ldots, \alpha_m \in K\} = \{\varphi(x) : x \in U\} = \text{Im}(\varphi),
  $$
  так как $U$ — множество всевозможных линейных комбинаций векторов своего базиса.



• Непосредственно из [Теорем 2](#теорема-2) и [3](#теорма-3) можно сделать следующее заключение.
## Следствие 1

Пусть $U$, $V$ — конечномерные линейные пространства над полем $K$, в которых фиксированы базисы, $\dim(U) = m$, а $\varphi : U \to V$ — линейное отображение с матрицей $A$. Тогда:
$$
\dim(\ker(\varphi)) = m - \text{rk}(A).
$$

- Покажем, как применять соответствие матриц и линейных отображений для доказательства утверждения "чисто про матрицы".

### Определение

Пусть $\varphi : U \to V$ — линейное отображение, а $W$ — линейное подпространство $U$. Тогда $\varphi|_W : W \to V$ — сужение $\varphi$ на подпространство $W$ (то же самое отображение, применяемое только для элементов $W$).

- Очевидно, $\varphi|_W$ — линейное отображение и $\text{Im}(\varphi|_W) \subset \text{Im}(\varphi)$.

# Ранг произведения матриц не превосходит рангов сомножиетелей
## Теорема 4


Пусть $A \in M_{m,n}(K)$, $B \in M_{n,\ell}(K)$. Тогда:
$$
\text{rk}(AB) \leq \min(\text{rk}(A), \text{rk}(B)).
$$

### Доказательство

- Рассмотрим линейные отображения:
  - $\varphi : K^n \to K^m$, заданное формулой $\varphi(x) = Ax$,
  - $\psi : K^\ell \to K^n$, заданное формулой $\psi(y) = By$.
- Как мы знаем, эти отображения имеют в стандартных базисах пространств матрицы $A$ и $B$ соответственно.
- По [Теореме 1](#теорема-1) композиция $\varphi \cdot \psi : K^\ell \to K^m$ — линейное отображение с матрицей $AB$.
- По [Теореме 3](#теорема-3) мы имеем:
  $$
  \text{rk}(B) = \dim(\text{Im}(\psi)), \quad \text{rk}(A) = \dim(\text{Im}(\varphi)), \quad \text{rk}(AB) = \dim(\text{Im}(\varphi \cdot \psi)).
  $$
- Нам достаточно доказать два неравенства:
  - $\text{rk}(AB) \leq \text{rk}(A)$,
  - $\text{rk}(AB) \leq \text{rk}(B)$.

#### Утверждение 1: $\text{rk}(AB) \leq \text{rk}(A)$

- Отметим, что $\varphi \cdot \psi = \varphi|_{\text{Im}(\psi)}$.
- Поэтому $\text{Im}(\varphi \cdot \psi) \subset \text{Im}(\varphi)$, а значит:
  $$
  \dim(\text{Im}(\varphi \cdot \psi)) \leq \dim(\text{Im}(\varphi)).
  $$

#### Утверждение 2: $\text{rk}(AB) \leq \text{rk}(B)$

- Так как $\varphi \cdot \psi = \varphi|_{\text{Im}(\psi)} : \text{Im}(\psi) \to K^m$, мы по [Теореме 2](#теорема-2) имеем:
  $$
  \dim(\text{Im}(\varphi \cdot \psi)) + \dim(\ker(\varphi \cdot \psi)) = \dim(\text{Im}(\psi)).
  $$
- Откуда:
  $$
  \text{rk}(AB) = \dim(\text{Im}(\varphi \cdot \psi)) \leq \dim(\text{Im}(\psi)) = \text{rk}(B).
  $$

- Теорема 4 доказана.

# Кольцо линейных операторов End(V), связь с кольцом матриц.
## Кольцо линейных операторов $End(V)$, связь с кольцом матриц

### Определение

Пусть $V$ — линейное пространство над полем $K$. Тогда $End(V)$ — множество всех линейных отображений из $V$ в себя, которые мы будем называть линейными операторами на $V$.

- В $End(V)$ мы определим сложение (поэлементное: $(\varphi + \psi)(x) := \varphi(x) + \psi(x)$) и умножение — композицию.
- Пусть $\dim(V) = n$. Зафиксируем базис $V$, каждому линейному оператору $\varphi \in End(V)$, как мы знаем, соответствует матрица $A_\varphi \in M_n(K)$ в этом базисе.
- Далее мы не будем говорить про базис, все матрицы в этом разделе именно в нем.

- Опишем несколько свойств биекции оператор — матрица. Первые два свойства очевидны.

### Свойство 1

Пусть $\varphi \in End(V)$ и $k \in K$. Тогда $k\varphi \in End(V)$ (это отображение, заданное формулой $(k\varphi)(x) := k\varphi(x)$) и $A_{k\varphi} = k \cdot A_\varphi$ (все элементы матрицы $A_\varphi$ умножаются на $k$).

### Свойство 2

Пусть $\varphi, \psi \in End(V)$. Тогда $A_{\varphi + \psi} = A_\varphi + A_\psi$.

- Третье свойство следует из [Теоремы 1](#теорема-1) (о матрице композиции линейных отображений).

### Свойство 3

Пусть $\varphi, \psi \in End(V)$. Тогда $A_{\varphi \cdot \psi} = A_\varphi \cdot A_\psi$.

## Теорема 5

Пусть $V$ — линейное пространство над полем $K$, $\dim(V) = n$. Тогда $End(V)$ — кольцо с единицей, причем $End(V) \cong M_n(K)$.

### Доказательство

- Мы знаем, что отображение $f : End(V) \to M_n(K)$, заданное формулой $f(\varphi) := A_\varphi$, является биекцией и согласовано с операциями $+$ и $\cdot$ в кольцах.
- Очевидно, $f^{-1}$ также согласовано с операциями в кольцах.
- Следовательно, все нужные нам свойства сложения (коммутативность, ассоциативность, $0$ и обратный элемент) в $End(V)$ следуют из аналогичных свойств в $M_n(K)$.
- Так, $0$ в $End(V)$ — это отображение с нулевой матрицей, которое отображает все элементы $V$ в $0$, а $-\varphi$ задается в каждой точке формулой $(-\varphi)(x) := -(\varphi(x))$.
- Аналогично, дистрибутивность и все нужные нам свойства умножения в $End(V)$ следуют из аналогичных свойств в $M_n(K)$.
- Отметим, что единицей в $End(V)$ будет, как и положено, тождественное отображение $id$ с матрицей $A_{id} = E_n$.
- Теперь понятно, что $f$ — изоморфизм колец $End(V)$ и $M_n(K)$.

# Обратимые линейные операторы и их свойства.
## Определение

Пусть $V$ — конечномерное линейное пространство над полем $K$, в котором зафиксирован базис. Тогда каждому линейному оператору $\varphi \in \text{End}(V)$ соответствует матрица $A_\varphi \in M_n(K)$, и это соответствие — биекция.

### Определение обратимого оператора

Линейный оператор $\varphi \in \text{End}(V)$ называется обратимым, если существует $\varphi^{-1}$ (то есть, если $\varphi$ — биекция).

### Лемма 2

Пусть $\varphi, \psi \in \text{End}(V)$ таковы, что $A_\psi = (A_\varphi)^{-1}$. Тогда $\varphi$ — обратимый и $\psi = \varphi^{-1}$.

#### Доказательство

- По [Теореме 1](#теорема-1) оператор $\varphi \cdot \psi \in \text{End}(V)$ имеет матрицу:
  $$
  A_{\varphi \cdot \psi} = A_\varphi \cdot A_\psi = E_n.
  $$
- Следовательно, $\varphi \cdot \psi = \text{id}$. Аналогично, $\psi \cdot \varphi = \text{id}$.
- Значит, $\psi = \varphi^{-1}$.

## Теорема 6

Если $\varphi \in \text{End}(V)$ — обратимый, то $\varphi^{-1} \in \text{End}(V)$ и $A_{\varphi^{-1}} = (A_\varphi)^{-1}$.

### Доказательство

- Для доказательства линейности $\varphi^{-1}$ достаточно проверить, что для любых $a, b \in K$ и $x, y \in V$ выполняется:
  $$
  \varphi^{-1}(a x + b y) = a \varphi^{-1}(x) + b \varphi^{-1}(y).
  $$
- Так как $\varphi$ — биекция, для этого достаточно доказать, что, применив к левой и правой частям этого равенства оператор $\varphi$, мы получим одно и то же:
  $$
  \varphi(\varphi^{-1}(a x + b y)) = a x + b y,
  $$
  $$
  \varphi(a \varphi^{-1}(x) + b \varphi^{-1}(y)) = a \varphi(\varphi^{-1}(x)) + b \varphi(\varphi^{-1}(y)) = a x + b y.
  $$
- Остается заметить, что:
  $$
  A_\varphi \cdot A_{\varphi^{-1}} = A_{\varphi \cdot \varphi^{-1}} = A_{\text{id}} = E_n,
  $$
  и аналогично:
  $$
  A_{\varphi^{-1}} \cdot A_\varphi = E_n,
  $$
  откуда следует, что:
  $$
  A_{\varphi^{-1}} = (A_\varphi)^{-1}.
  $$

# Координаты вектора в разных базисах. Матрицы перехода и их свойства

## Координаты вектора в разных базисах

- Пусть $V$ — линейное пространство над полем $K$, а $e_1, \ldots, e_n$ и $e'_1, \ldots, e'_n$ — два его базиса.
- Каждый вектор $x \in V$ единственным образом представляется в виде столбцов координат по каждому из базисов — скажем, $(x_1, \ldots, x_n)^T$ и $(x'_1, \ldots, x'_n)^T$.
- Мы покажем, как из столбца координат вектора по одному базису получить столбец координат по другому базису.
- Разложим базисные векторы второго базиса по первому:
  $$
  e'_1 = c_{1,1}e_1 + c_{2,1}e_2 + \cdots + c_{n,1}e_n, \ldots,
  $$
  $$
  e'_i = c_{1,i}e_1 + c_{2,i}e_2 + \cdots + c_{n,i}e_n, \ldots,
  $$
  $$
  e'_n = c_{1,n}e_1 + c_{2,n}e_2 + \cdots + c_{n,n}e_n.
  $$
- Напишем цепочку преобразований:
  $$
  x_1e_1 + \cdots + x_ne_n = x = x'_1e'_1 + \cdots + x'_ne'_n =
  $$
  $$
  x'_1(c_{1,1}e_1 + c_{2,1}e_2 + \cdots + c_{n,1}e_n) + \cdots + x'_n(c_{1,n}e_1 + c_{2,n}e_2 + \cdots + c_{n,n}e_n) =
  $$
  $$
  (x'_1c_{1,1} + \cdots + x'_nc_{1,n})e_1 + \cdots + (x'_1c_{n,1} + \cdots + x'_nc_{n,n})e_n,
  $$
  откуда, ввиду единственности разложения по базису, делаем вывод $x_i = (c_{i,1}x'_1 + \cdots + c_{i,n}x'_n)$.
- Вспомнив про правила умножения матриц, получаем формулу:
  $$
  (x_1, \ldots, x_n)^T = C(x'_1, \ldots, x'_n)^T, \text{ где } C = (c_{i,j})_{1 \leq i \leq n, 1 \leq j \leq n}.
  $$
- Матрица $C$ называется матрицей перехода от базиса $e'_1, \ldots, e'_n$ к базису $e_1, \ldots, e_n$. Докажем ряд свойств матриц перехода.

#### Свойство 1

- Матрица перехода невырождена. Если $C$ — матрица перехода от базиса $e'_1, \ldots, e'_n$ к базису $e_1, \ldots, e_n$, то матрица перехода от базиса $e_1, \ldots, e_n$ к базису $e'_1, \ldots, e'_n$ — это $C^{-1}$.

##### Доказательство

- Обозначим матрицу перехода от базиса $e_1, \ldots, e_n$ к базису $e'_1, \ldots, e'_n$ через $D$.
- Тогда для любого $x \in V$ мы можем записать его координаты в этих базисах, как выше, и заметить, что:
  $$
  (x_1, \ldots, x_n)^T = C(x'_1, \ldots, x'_n)^T \text{ и } (x'_1, \ldots, x'_n)^T = D(x_1, \ldots, x_n)^T,
  $$
  откуда:
  $$
  (x_1, \ldots, x_n)^T = C(x'_1, \ldots, x'_n)^T = CD(x_1, \ldots, x_n)^T,
  $$
  $$
  (x'_1, \ldots, x'_n)^T = D(x_1, \ldots, x_n)^T = DC(x'_1, \ldots, x'_n)^T.
  $$
- Так как это верно для любых столбцов координат, $DC = CD = E_n$ (умножение на $CD$ или $DC$ — это $\text{id}$), откуда следует, что $D = C^{-1}$.

#### Свойство 2

- Пусть $V$ — линейное пространство над полем $K$, $\dim(V) = n$. Тогда для любого фиксированного базиса $e_1, \ldots, e_n$, любая невырожденная матрица $C \in M_n(K)$ является матрицей перехода от какого-то базиса к $e_1, \ldots, e_n$.

##### Доказательство

- Рассмотрим базис $e'_1, \ldots, e'_n$, координаты векторов которого в исходном базисе $e_1, \ldots, e_n$ — столбцы матрицы $C$ (то есть, $e'_1 = C^{(1)}, \ldots, e'_n = C^{(n)}$).
- Эти вектора линейно независимы: так как $C$ — невырожденная матрица, то:
  $$
  \dim(V) = n = \text{rk}(C) = \dim(\text{Lin}(C^{(1)}, \ldots, C^{(n)})).
  $$
- Любые $n$ линейно независимых векторов в $n$-мерном пространстве $V$ образуют его базис, в частности, $e'_1, \ldots, e'_n$ — базис $V$.
- По построению матрицы перехода понятно, что матрица перехода от базиса $e'_1, \ldots, e'_n$ к базису $e_1, \ldots, e_n$ будет в точности $C$.

# Матрицы оператора в разных базисах. Свойства подобных матриц
  ## Матрицы оператора в разных базисах

- Пусть $\varphi \in \text{End}(V)$, где $V$ — линейное пространство над полем $K$, $\dim(V) = n$.
- Мы знаем, что при каждом фиксированном базисе оператору $\varphi$ соответствует его матрица. Как связаны матрицы одного оператора в разных базисах?
- Пусть $e_1, \ldots, e_n$ и $e'_1, \ldots, e'_n$ — два базиса $V$, а векторам соответствуют столбцы координат (без штрихов — по первому базису, со штрихами — по второму).
- Пусть $A$ и $A'$ — матрицы $\varphi$ в этих базисах. Тогда равенство $y = \varphi(x)$ можно переписать в двух видах:
  $$
  (y_1, \ldots, y_n)^T = A \cdot (x_1, \ldots, x_n)^T,
  $$
  $$
  (y'_1, \ldots, y'_n)^T = A' \cdot (x'_1, \ldots, x'_n)^T.
  $$
- Пусть $C$ — матрица перехода от $e'_1, \ldots, e'_n$ к $e_1, \ldots, e_n$.
- Тогда $C^{-1}$ — матрица перехода от $e_1, \ldots, e_n$ к $e'_1, \ldots, e'_n$.

- Поэтому:
  $$
  (y'_1, \ldots, y'_n)^T = A'C^{-1}(x_1, \ldots, x_n)^T,
  $$
  $$
  (y_1, \ldots, y_n)^T = C(y'_1, \ldots, y'_n)^T = CA'C^{-1}(x_1, \ldots, x_n)^T.
  $$
- Ввиду единственности матрицы отображения $\varphi$ в базисе $e_1, \ldots, e_n$ следует, что:
  $$
  A = CA'C^{-1}, \quad A' = C^{-1}AC.
  $$

### Определение

- Матрицы $A, A' \in M_n(K)$ называются **подобными**, если существует такая невырожденная матрица $C \in M_n(K)$, что:
  $$
  A = CA'C^{-1}.
  $$
- Несложно понять, что подобие матриц — отношение эквивалентности. Таким образом, все матрицы из $M_n(K)$ разбиваются на классы эквивалентности, состоящие из попарно подобных матриц.

## Теорема 7

- Пусть $\varphi \in \text{End}(V)$, где $V$ — линейное пространство над полем $K$, $\dim(V) = n$. Тогда матрицы $\varphi$ во всех возможных базисах образуют класс попарно подобных матриц из $M_n(K)$.

### Доказательство

- Зафиксируем базис $e_1, \ldots, e_n$ и матрицу $A$ отображения $\varphi$ в этом базисе.
- Уже доказано, что матрицы $\varphi$ в других базисах подобны $A$.
- Рассмотрим произвольную матрицу, подобную $A$ — скажем, $C^{-1}AC$, где $C \in M_n(K)$ — невырожденная матрица.
- Тогда $C$ — это матрица перехода от нашего фиксированного базиса к другому, и в этом базисе $\varphi$ имеет матрицу как раз $C^{-1}AC$.

# Многочлен от оператора и от матрицы, соответствие между ними

- Пусть $V$ — линейное пространство над полем $K$, $\dim(V) = n$, базис считаем фиксированным.
- Пусть $\varphi \in \text{End}(V)$, а $A \in M_n(K)$ — матрица $\varphi$.
- Рассмотрим многочлен $f(t) \in K[t]$, пусть
  $$
  f(t) = c_m t^m + \cdots + c_0.
  $$
- Мы научимся подставлять в многочлен $f$ матрицу $A$ и линейный оператор $\varphi$:
  $$
  f(A) := c_m A^m + \cdots + c_1 A + c_0 E_n,
  $$
  $$
  f(\varphi) := c_m \varphi^m + \cdots + c_1 \varphi + c_0 \text{id}.
  $$
- $f(A)$ — матрица из $M_n(K)$. Умножение матрицы на число (выражение типа $c_1 A$) — это умножение всех её коэффициентов на это число.
- $f(\varphi)$ — это линейный оператор из $\text{End}(V)$. Результат умножения оператора на число (выражение типа $c_1 \varphi$) — это оператор, значения которого во всех точках умножены на это число.

## Лемма 3 
Оператор $f(φ)$ в нашем фиксированном базисе имеет матрицу $f(A)$.

### Доказательство

- При сложении линейных отображений матрицы складываются, при композиции — перемножаются.
- При умножении линейного отображения на число $c \in K$ все его значения умножаются на это число, а значит, и значения на базисных векторах, но тогда и матрица отображения умножится на $c$.

- Умножение в $M_n(K)$ и в $End(V)$ не коммутативно. Но многочлены от одного и того же оператора (матрицы) — коммутируют.

## Лемма 4

- Пусть $f, g \in K[t]$.
  1. Для любого $\varphi \in \text{End}(V)$ выполнено:
     $$
     f(\varphi) \cdot g(\varphi) = g(\varphi) \cdot f(\varphi).
     $$
  2. Для любой $A \in M_n(K)$ выполнено:
     $$
     f(A) \cdot g(A) = g(A) \cdot f(A).
     $$

### Доказательство

- Ввиду Леммы 3 достаточно доказать пункт 2.
- Пусть $f(t) = c_m t^m + \cdots + c_0$ и $g(t) = d_k t^k + \cdots + d_0$.
- Так как
  $$
  (c_i A^i) \cdot (d_j A^j) = c_i d_j A^{i+j} = d_j c_i A^{i+j} = (d_j A^j) \cdot (c_i A^i),
  $$
  можно написать цепочку преобразований (везде считаем, что $A^0 = E_n$):
  $$
  f(A) g(A) =
  \left( \sum_{i=0}^m c_i A^i\right)  \cdot \left( \sum_{j=0}^k d_j A^j \right) = \\
  \sum_{i=0}^m \sum_{j=0}^k (c_i A^i)(d_j A^j) =
  \sum_{j=0}^k \sum_{i=0}^m (d_j A^j)(c_i A^i) = \\
  \left( \sum_{j=0}^k d_j A^j \right) \cdot \sum_{i=0}^m c_i A^i = g(A) f(A).
  $$

# Инвариантные подпространства.

- Пусть $V$ — линейное пространство над полем $K$, а $\varphi \in \text{End}(V)$.

## Определение

- Подпространство $W < V$ называется $\varphi$-инвариантным, если
  $$
  \varphi(W) = \{\varphi(x) : x \in W\} \subset W.
  $$

- Вскоре мы увидим много примеров инвариантных подпространств и поймем, насколько это важное понятие. Начнем с простого свойства.

## Свойство

- Если $W$ — $\varphi$-инвариантное подпространство, то $\varphi|_W \in \text{End}(W)$.

### Доказательство

- По определению $\varphi$-инвариантного подпространства, $\varphi|_W : W \to W$.
- Условие линейности наследуется от $\varphi$.

## Определение

Пусть $K$ — поле, $A_1, \ldots, A_m$ — квадратные матрицы, $A_i \in M_{n_i}(K)$. Обозначим через $\text{diag}(A_1, \ldots, A_m)$ квадратную матрицу из $M_{n_1 + \cdots + n_m}(K)$, в которой по главной диагонали последовательно стоят квадратные блоки $A_1, \ldots, A_m$, а все остальные элементы равны $0$.

![Рисунок a](https://github.com/wwwyssa/wwwyssa_in_itmo/blob/main/sexia_part2/algebra/3.png?raw=true)


## Лемма 5

Пусть $V$ — линейное пространство над полем $K$, $\varphi \in \text{End}(V)$, а $V_1, \ldots, V_m$ — $\varphi$-инвариантные подпространства $V$, причем
$$
V = \bigoplus_{i=1}^m V_i.
$$

Зафиксируем в каждом подпространстве $V_i$ базис $e^i_1, \ldots, e^i_{k_i}$, пусть $A_i$ — матрица отображения $\varphi_i = \varphi|_{V_i}$ в этом базисе. Тогда в базисе $e^1_1, \ldots, e^1_{k_1}, \ldots, e^m_1, \ldots, e^m_{k_m}$ матрица $\varphi$ имеет вид $\text{diag}(A_1, \ldots, A_m)$.

### Доказательство

- Как строится матрица $A$ отображения $\varphi$ в некотором базисе $e_1, \ldots, e_n$? Её $j$-й столбец будет $A^{(j)} = \varphi(e_j)$.
- Пусть $j^i$ — номер базисного вектора $e^i_j$, входящего в базис $\varphi$-инвариантного подпространства $V_i$. Подставим его и получим:
  $$
  A^{(j^i)} = \varphi(e^i_j) \in V_i.
  $$
- Элемент из $V_i$ раскладывается по его базису $e^i_1, \ldots, e^i_{k_i}$, а значит, в разложении по объединенному базису пространства $V$ мы получим коэффициенты $0$ при всех векторах, кроме этих (так как разложение по базису единственно!).
- Следовательно:
  $$
  A^{(j^i)} = \varphi(e^i_j) = \sum_{s=1}^{k_i} a_{s^i, j^i} e_{s^i} = (0, \ldots, 0, a^i_{1, j_i}, \ldots, a^i_{k_i, j_i}, 0, \ldots, 0)^T.
  $$
- Так как $\varphi(e^i_j) = \varphi_i(e^i_j)$, на места столбцов, соответствующих векторам из базиса $e^i_1, \ldots, e^i_{k_i}$, будут вписаны соответствующие столбцы матрицы $A_i$ (эти столбцы как раз и образованы коэффициентами в разложении векторов $\varphi(e^i_j)$ по указанному базису пространства $V_i$), а все остальные коэффициенты матрицы $A$ в указанных столбцах — нули.

# Характеристический многочлен оператора. Корректность определения, свойства.

## Характеристический многочлен оператора

- Пусть $V$ — линейное пространство над полем $K$, $\dim(V) = n$, а $\varphi \in \text{End}(V)$.

### Определение

- Пусть $A$ — любая матрица оператора $\varphi$.
- **Характеристический многочлен** оператора $\varphi$ — это
  $$
  \chi_\varphi(t) := \det(A - tE_n),
  $$
  где $t$ — переменная из поля $K$, $\chi_\varphi \in K[t]$.

### Свойство 1

- Определение характеристического многочлена оператора корректно, то есть, не зависит от выбора матрицы оператора.

#### Доказательство

- Пусть $A'$ — другая матрица оператора $\varphi$.
- Тогда $A' = C^{-1}AC$ для некоторой невырожденной матрицы $C \in M_n(K)$, следовательно:
  $$
  \det(A' - tE_n) = \det(C^{-1}AC - tE_n) = \det(C^{-1}AC - C^{-1}(tE_n)C) =
  \det(C^{-1}(A - tE_n)C) = \det(C^{-1})\det(A - tE_n)\det(C) = \det(A - tE_n).
  $$

### Свойство 2

- $\deg(\chi_\varphi) = n$, старший коэффициент равен $(-1)^n$, а свободный член равен $\det(A)$.

#### Доказательство

- По диагонали матрицы $A - tE_n$ стоят коэффициенты $a_{i,i} - t$, остальные коэффициенты $t$ не содержат.
- Следовательно, $t$ может быть максимум в степени $n$, и в такой степени получается ровно в одном случае — в произведении диагональных элементов, с коэффициентом $(-1)^n$.
- Для вычисления свободного члена подставим $t = 0$ и получим $\det(A)$.
- Итак:
  $$
  \chi_\varphi(t) = (-1)^n t^n + c_{n-1}t^{n-1} + \cdots + c_1t + \det(A).
  $$

### Определение

- След матрицы $A \in M_n(K)$ — это $\text{Tr}(A) = a_{1,1} + a_{2,2} + \cdots + a_{n,n}$ (сумма элементов на главной диагонали).

### Свойство 3

- $c_{n-1} = (-1)^{n-1}\text{Tr}(A)$.

#### Доказательство

- Пусть $B = A - tE_n$, а элементы этой матрицы обозначим через $b_{i,j}$.
- Рассмотрим любое произведение $b_{1,\sigma(1)} \cdots b_{n,\sigma(n)}$, входящее в $\det(B) = \chi_\varphi(t)$ (здесь $\sigma \in S_n$).
- В этом произведении не может быть ровно один элемент, не лежащий на главной диагонали, так как $\sigma$ не может оставлять на месте все числа от $1$ до $n$, кроме одного.
- Следовательно, если $\sigma = \text{id}$, то произведение содержит переменную $t$ в степени не более $n - 2$.
- Значит, вклад в $c_{n-1}t^{n-1}$ дает только произведение диагональных элементов, равное:
  $$
  (a_{1,1} - t)(a_{2,2} - t)\cdots(a_{n,n} - t),
  $$
  откуда ясно, что $c_{n-1} = (-1)^{n-1}\text{Tr}(A)$.

### Свойство 4

- Если матрицы $A$ и $A'$ подобны, то $\text{Tr}(A) = \text{Tr}(A')$.

#### Доказательство

- Выше доказано (см. Свойство 1), что тогда:
  $$
  \det(A - tE_n) = \det(A' - tE_n),
  $$
  откуда ввиду Свойства 3 следует, что $\text{Tr}(A) = \text{Tr}(A')$.

# Теорема Гамильтона-Кэли.

## Теорема Гамильтона-Кэли

- Пусть $V$ — конечномерное линейное пространство над полем $K$, $\dim(V) = n$, $\varphi \in \text{End}(V)$. Тогда:
  $$
  \chi_\varphi(\varphi) = 0.
  $$

### Доказательство Теоремы 8

- Обозначим через $B(t)$ взаимную матрицу к $A - tE_n$ (здесь $t$ — переменная, принимающая значения из поля $K$).
- Тогда $b(t)_{i,j} = (A - tE_n)_{j,i}$ — минор порядка $n - 1$ матрицы $A - tE_n$, который, очевидно, является многочленом от $t$ степени не более чем $n - 1$ (в подматрице $A - tE_n$, полученной вычеркиванием $i$-й строки и $j$-го столбца, остается не более чем $n - 1$ элемент вида $a_{l,l} - t$, остальные не содержат переменную $t$).
- Поэтому существуют такие матрицы $B_0, \ldots, B_{n-1} \in M_n(K)$, что:
  $$
  B(t) = B_{n-1}t^{n-1} + \cdots + B_1t + B_0.
  $$
- Пусть $\chi_\varphi(t) = c_nt^n + \cdots + c_1t + c_0$ (нам известно, что $\deg(\chi_\varphi) = \dim(V) = n$). Тогда:
  $$
  (c_nt^n + \cdots + c_1t + c_0)E_n = \chi_\varphi(t) \cdot E_n = \det(A - tE_n) \cdot E_n = B(t) \cdot (A - tE_n).
  $$
- Подставляя $B(t) = B_{n-1}t^{n-1} + \cdots + B_1t + B_0$ и раскрывая произведение, получаем:
  $$
  B(t) \cdot (A - tE_n) = (B_{n-1}t^{n-1} + \cdots + B_1t + B_0) \cdot (A - tE_n).
  $$
 - В левой и правой частях равенства:  
  $$  
  (c_nt^n + \cdots + c_1t + c_0)E_n = (B_{n-1}t^{n-1} + \cdots + B_1t + B_0) \cdot (A - tE_n),  
  $$  
  коэффициенты при каждой степени $t$ должны совпадать:  
  $$  
  c_nE_n = -B_{n-1}, \rightarrow c_nA^n=-B_{n-1}A^n \\ c_{n-1}E_n = B_{n-1}A - B_{n-2}, \rightarrow c_{n-1}A^{n-1}=B_{n-1}A^n-B_{n-2}A^{n-1}\\ \quad \ldots, \\
  c_1E_n = B_1A - B_0 \rightarrow c_1A^1=b_1A^2-B_0A, \\ c_0E_n = B_0A.  
  $$  
  
- Сложив равенства, получим:  
  $$  
  \chi_\varphi(A) = c_nA^n + c_{n-1}A^{n-1} + \cdots + c_1A + c_0E_n = -B_{n-1}A^n + (B_{n-1}A^n - B_{n-2}A^{n-1}) + \cdots + (B_1A^2 - B_0A) + B_0A = 0.  
  $$  
  
- Таким образом, оператор $\chi_\varphi(\varphi)$ имеет нулевую матрицу, а значит, $\chi_\varphi(\varphi) = 0$.

# Минимальный многочлен оператора.

## Минимальный многочлен оператора

- Пусть $V$ — линейное пространство над полем $K$, $\dim(V) = n$, $\psi \in \text{End}(V)$.
- Пусть $I(\psi)$ — множество всех таких многочленов $f \in K[t]$, что $f(\psi) = 0$.

### Свойство 1

- $I(\psi)$ — идеал в $K[t]$.

#### Доказательство

- Достаточно проверить замкнутость $I(\psi)$ по сложению и умножению на многочлены из $K[t]$.
- Пусть $f, g \in I(\psi)$. Тогда:
  $$
  (f + g)(\psi) = f(\psi) + g(\psi) = 0 + 0 = 0,
  $$
  а значит, $f + g \in I(\psi)$.
- Пусть $f \in I(\psi)$, $h \in K[t]$. Тогда:
  $$
  (fh)(\psi) = f(\psi) \cdot h(\psi) = 0 \cdot h(\psi) = 0,
  $$
  а значит, $fh \in I(\psi)$.
- Как мы знаем, любой идеал в $K[t]$ — главный. Рассмотрим любой многочлен $f$ такой, что $I(\psi) = f \cdot K[t]$ (то есть, порождающий $I(\psi)$).
- Тогда для любого другого многочлена $g \in K[t]$, порождающего $I(\psi)$, очевидно, выполнено $f \mid g$ и $g \mid f$, то есть, эти два многочлена ассоциированы (отличаются умножением на константу).
- Следовательно, многочлены, порождающие $I(\psi)$ — это в точности многочлены вида $cf$. Ровно один из них имеет единичный старший коэффициент, мы обозначим его через $\text{Irr}_\psi$ и будем называть **минимальным многочленом** оператора $\psi$.

#### Свойство 2

- $\chi_\psi \vdots\text{Irr}_\psi$.

##### Доказательство

- По теореме Гамильтона-Кэли мы знаем, что:
  $$
  \chi_\psi(\psi) = 0,
  $$
  то есть, $\chi_\psi \in I(\psi)$.
- Все многочлены из идеала $I(\psi)$ делятся на $\text{Irr}_\psi$.

# Собственные числа, векторы и подпространства. Связь с характеристическим многочленом.

## Собственные числа, векторы и подпространства

- Пусть $V$ — линейное пространство над полем $K$, $\dim(V) = n$, $\psi \in \text{End}(V)$.

### Определение

1. Число $\lambda \in K$ называется **собственным числом** оператора $\psi$, если существует такой ненулевой вектор $x \in V$, что:
   $$
   \psi(x) = \lambda \cdot x.
   $$
   В этом случае говорят, что $x$ — **собственный вектор** числа $\lambda$.
2. Пусть $\lambda$ — собственное число оператора $\psi$. Множество $V_\lambda$, состоящее из всех собственных векторов числа $\lambda$ и вектора $0$, называется **собственным подпространством** числа $\lambda$.
3. Множество всех собственных чисел оператора $\psi$ называется его **спектром** и обозначается через $\text{Spec}(\psi)$.

### Лемма 6

- $V_\lambda$ — линейное подпространство $V$.

#### Доказательство

- Пусть $x, y \in V_\lambda$, $\alpha, \beta \in K$.
- Нам достаточно проверить, что $\alpha x + \beta y \in V_\lambda$. Сделаем это:
  $$
  \psi(\alpha x + \beta y) = \alpha \psi(x) + \beta \psi(y) = \alpha \lambda x + \beta \lambda y = \lambda (\alpha x + \beta y).
  $$

### Теорема 9

- $\text{Spec}(\psi)$ состоит в точности из корней характеристического многочлена $\chi_\psi(t)$.

#### Доказательство

- Зафиксируем базис $V$, пусть $A$ — матрица $\psi$ в этом базисе.
- Пусть $\lambda \in \text{Spec}(\psi)$, $x \in V_\lambda$, $x \neq 0$. Тогда:
  $$
  \psi(x) = \lambda x = \lambda \cdot \text{id}(x).
  $$
- Следовательно:
  $$
  (\psi - \lambda \cdot \text{id})(x) = 0,
  $$
  то есть $x \in \ker(\psi - \lambda \cdot \text{id})$.
- Значит, $\ker(\psi - \lambda \cdot \text{id}) \neq \{0\}$ и $\dim(\ker(\psi - \lambda \cdot \text{id})) > 0$.
- Оператор $\psi - \lambda \cdot \text{id}$ имеет матрицу $A - \lambda E_n$, и по [Следствию 1](#следствие-1):
  $$
  \text{rk}(A - \lambda E_n) = n - \dim(\ker(\psi - \lambda \cdot \text{id})) < n.
  $$
- Тогда по Следствию 7.2 имеем:
  $$
  \chi_\psi(\lambda) = \det(A - \lambda E_n) = 0.
  $$

- Наоборот, пусть $\chi_\psi(\lambda) = \det(A - \lambda E_n) = 0$.
- Тогда по Следствию 7.2:
- - Матрица $A∈M_n(K)$ обратима, если и только если $rk(A)=n$.
  $$
  n > \text{rk}(A - \lambda E_n) = n - \dim(\ker(\psi - \lambda \cdot \text{id})) \implies \dim(\ker(\psi - \lambda \cdot \text{id})) > 0.
  $$
- Следовательно, существует ненулевой вектор $x \in \ker(\psi - \lambda \cdot \text{id})$.
- Это означает, что:
  $$
  \psi(x) = \lambda x,
  $$
  то есть $\lambda \in \text{Spec}(\psi)$.

# Линейная независимость собственных векторов разных собственных чисел. Сумма собственных пространств — прямая.

## Теорема 10

- Пусть $V$ — линейное пространство над полем $K$, $\psi \in \text{End}(V)$, $\lambda_1, \ldots, \lambda_k \in \text{Spec}(\psi)$ — различные собственные числа, а $x_i$ — собственный вектор $\lambda_i$. Тогда $x_1, \ldots, x_k$ линейно независимы.

### Доказательство

- Предположим противное и найдем из этих векторов нетривиальную нулевую линейную комбинацию с минимальным количеством ненулевых коэффициентов:
  $$
  \alpha_1 x_1 + \cdots + \alpha_s x_s = 0. \tag{*}
  $$
- Подействуем на левую и правую часть $(*)$ оператором $\psi$:
  $$
  0 = \psi(0) = \psi(\alpha_1 x_1 + \cdots + \alpha_s x_s) = \alpha_1 \psi(x_1) + \cdots + \alpha_s \psi(x_s) = \lambda_1 \alpha_1 x_1 + \cdots + \lambda_s \alpha_s x_s. (**)
  $$
- Вычтем из $(**)$ умноженное на $\lambda_s$ равенство $(*)$ и получим:
  $$
  \alpha_1 (\lambda_1 - \lambda_s) x_1 + \cdots + \alpha_{s-1} (\lambda_{s-1} - \lambda_s) x_{s-1} = 0.
  $$
- Так как собственные числа различны, все коэффициенты в этой линейной комбинации отличны от $0$, но тогда ее существование противоречит выбору $(*)$.

## Следствие 2

- Пусть $V$ — линейное пространство над полем $K$, $\psi \in \text{End}(V)$, $\text{Spec}(\psi) = \{\lambda_1, \ldots, \lambda_k\}$. Тогда:
  $$
  V = \bigoplus_{i=1}^k V_{\lambda_i}.
  $$

### Доказательство

- На этот раз нам понадобится определение прямой суммы: нужно доказать, что равенство
  $$
  0 = \sum_{i=1}^k x_i, \quad x_i \in V_{\lambda_i},
  $$
  возможно только при $x_1 = \cdots = x_k = 0$.
- Это очевидно следует из [Теоремы 10](#теорема-10): в противном случае, несколько ненулевых векторов из разных собственных пространств были бы линейно зависимы.

# Диагонализируемые операторы и матрицы.
## Диагонализируемые операторы и матрицы

### Определение

- Пусть $V$ — линейное пространство над полем $K$, $\psi \in \text{End}(V)$.
 - Оператор $\psi$ — диагонализируемый, если в некотором базисе он имеет диагональную матрицу (то есть матрицу, в которой все элементы не на главной диагонали равны $0$).
  - Матрица $A \in M_n(K)$ называется диагонализируемой, если она имеет подобную диагональную матрицу.
  - Матрица $A \in M_n(K)$ является диагонализируемой, если и только если оператор умножения на $A$ в каком-либо базисе является диагонализируемым.

### Теорема 11

- Пусть $V$ — линейное пространство над полем $K$, $\psi \in \text{End}(V)$, $\text{Spec}(\psi) = \{\lambda_1, \ldots, \lambda_k\}$. Тогда следующие три утверждения равносильны:
  1. Оператор $\psi$ — диагонализируемый.
  2. $V = \bigoplus_{i=1}^k V_{\lambda_i}$.
  3. $V$ имеет базис, состоящий из собственных векторов $\psi$.

### Доказательство

#### $3^\circ \implies 1^\circ$

- Пусть $e_1, \ldots, e_n$ — базис $V$, причем $e_i$ — собственный вектор числа $\lambda_i$ (возможно, не все эти собственные числа различны).
- Тогда $\psi(e_i) = \lambda_i \cdot e_i$, поэтому матрица $\psi$ в этом базисе имеет вид $\text{diag}(\lambda_1, \ldots, \lambda_n)$.

#### $1^\circ \implies 3^\circ$

- Пусть матрица $\psi$ в базисе $e_1, \ldots, e_n$ — это $\text{diag}(\lambda_1, \ldots, \lambda_n)$.
- Тогда $\psi(e_i) = \lambda_i e_i$, причем, очевидно, $e_i \neq 0$. Следовательно, $\lambda_i$ — собственное число, а $e_i$ — его собственный вектор.

#### $2^\circ \implies 3^\circ$

- Пусть $V = \bigoplus_{i=1}^k V_{\lambda_i}$.
- Выделим базис в каждом из пространств $V_{\lambda_1}, \ldots, V_{\lambda_k}$, тогда каждый из этих базисов состоит из собственных векторов $\psi$, а объединение всех этих $k$ базисов по свойствам прямой суммы дает базис $V$.

#### $3^\circ \implies 2^\circ$

- Мы знаем, что $W = \bigoplus_{i=1}^k V_{\lambda_i}$ — линейное подпространство $V$ (эта сумма прямая по Следствию 2).
- Пусть $m_i = \dim(V_{\lambda_i})$. Тогда:
  $$
  \dim(V) \geq \dim(W) = \sum_{i=1}^k \dim(V_{\lambda_i}) = \sum_{i=1}^k m_i.
  $$
- С другой стороны, пусть $e_1, \ldots, e_n$ — базис $V$, состоящий из собственных векторов.
- Тогда для каждого $i \in \{1, \ldots, k\}$ в $V_{\lambda_i}$ лежит не более чем $m_i$ векторов из базиса (так как они линейно независимы).
- Значит:
  $$
  \dim(V) \leq \sum_{i=1}^k m_i,
  $$
  откуда следует, что $\dim(V) = \dim(W)$, а значит, $V = W$.

# Корневые подпространства. Свойства.
## Корневые подпространства

- Пусть $V$ — линейное пространство над полем $K$, $\dim(V) = n$, $\varphi \in \text{End}(V)$.
- Если прямая сумма собственных пространств оператора $\varphi$ равна $V$, в некотором базисе этот оператор имеет диагональную матрицу, с которой очень удобно иметь дело.
- А что же делать, когда эта сумма меньше $V$? Нам придется определить понятие, расширяющее собственное пространство.

### Определение

- Пусть $\lambda \in \text{Spec}(\varphi)$. Тогда:
  $$
  V(\lambda) = \{x \in V \mid \exists k \in \mathbb{N} : (\varphi - \lambda \cdot \text{id})^k(x) = 0\}
  $$
  — корневое пространство собственного числа $\lambda$ оператора $\varphi$.
- В этом определении много сложного и неудобного — например, есть квантор существования, от которого мы вскоре избавимся.

### Свойство 0

- $V(\lambda)$ — подпространство $V$.

#### Доказательство

- Пусть $x, y \in V(\lambda)$, $\alpha, \beta \in K$.
- Нам надо доказать, что $\alpha x + \beta y \in V(\lambda)$.
- По определению, существует такое $k \in \mathbb{N}$, что:
  $$
  (\varphi - \lambda \cdot \text{id})^k(x) = (\varphi - \lambda \cdot \text{id})^k(y) = 0,
  $$
  тогда:
  $$
  (\varphi - \lambda \cdot \text{id})^k(\alpha x + \beta y) = \alpha (\varphi - \lambda \cdot \text{id})^k(x) + \beta (\varphi - \lambda \cdot \text{id})^k(y) = 0,
  $$
  откуда следует, что $\alpha x + \beta y \in V(\lambda)$.

### Свойство 1

- Собственное пространство $V_\lambda$ — подпространство корневого пространства $V(\lambda)$.

#### Доказательство

- Достаточно доказать, что $V_\lambda \subset V(\lambda)$.
- Пусть $x \in V_\lambda$. Тогда $\varphi(x) = \lambda x$, откуда следует, что:
  $$
  (\varphi - \lambda \cdot \text{id})(x) = 0,
  $$
  то есть, подходит $k = 1$.

### Лемма 7

- Пусть $\psi \in \text{End}(V)$, $x \in V$. Пусть $k \in \mathbb{N}$ — минимальное такое число, что $\psi^k(x) = 0$. Тогда $x, \psi(x), \ldots, \psi^{k-1}(x)$ — линейно независимые векторы. В частности, $k \leq n$.

#### Доказательство

- Пусть $x, \psi(x), \ldots, \psi^{k-1}(x)$ линейно зависимы, то есть:
  $$
  \sum_{i=0}^{k-1} \alpha_i \psi^i(x) = 0, \quad \text{не все } \alpha_i = 0. \tag{*}
  $$
- Пусть $\ell$ — минимальный такой индекс, что $\alpha_\ell \neq 0$. Тогда сумму в $(*)$ можно начинать с индекса $\ell$.
- Применим к обеим частям равенства $(*)$ оператор $\psi^{k-1-\ell}$ и получим:
  $$
  0 = \psi^{k-1-\ell}(0) = \psi^{k-1-\ell} \left( \sum_{i=\ell}^{k-1} \alpha_i \psi^i(x) \right) = \alpha_\ell \psi^{k-1}(x) + \sum_{i=\ell+1}^{k-1} \alpha_i \psi^{k-1-\ell+i}(x).
  $$
- Последний переход верен, так как $k-1-\ell+i \geq k$ при $i \geq \ell+1$, а значит, $\psi^{k-1-\ell+i}(x) = 0$.
- Полученное равенство не может быть верным, так как $\alpha_\ell \neq 0$ и $\psi^{k-1}(x) = 0$.
- Поскольку в $n$-мерном линейном пространстве $V$ нельзя выбрать более $n$ линейно независимых векторов, $k \leq n$.

## Свойство 2

- Теперь мы готовы избавиться от квантора существования в определении корневого пространства:
  $$
  V(\lambda) = \{x \in V : (\varphi - \lambda \cdot \text{id})^n(x) = 0\}.
  $$

### Доказательство

- Из определения следует, что:
  $$
  V(\lambda) \supseteq \{x \in V : (\varphi - \lambda \cdot \text{id})^n(x) = 0\}.
  $$
- Наоборот, пусть $x \in V(\lambda)$.
- Рассмотрим минимальное такое $k \in \mathbb{N}$, что:
  $$
  (\varphi - \lambda \cdot \text{id})^k(x) = 0
  $$
  (такое $k$ существует по определению).
- По Лемме 7 мы имеем $k \leq n$.
- Значит, и:
  $$
  (\varphi - \lambda \cdot \text{id})^n(x) = 0.
  $$

# Лемма о двух взаимно простых операторных многочленах.
### Лемма 8

- Пусть $f, g \in K[t]$ — взаимно простые многочлены. Пусть $V$ — линейное пространство над полем $K$, а $x \in V$ и $\psi \in \text{End}(V)$ таковы, что:
  $$
  (f(\psi))(x) = 0 \quad \text{и} \quad (g(\psi))(x) = 0.
  $$
  Тогда $x = 0$.

#### Доказательство

- Вспомним, что НОД двух многочленов представляется в виде их линейной комбинации.
- Поэтому существуют такие многочлены $p, q \in K[t]$, что:
  $$
  fp + qg = 1.
  $$
- Подставив в это равенство оператор $\psi$, получим:
  $$
  (p f)(\psi) + (q g)(\psi) = \text{id}.
  $$
- Применим обе части последнего операторного равенства к вектору $x$:
  $$
  x = \text{id}(x) = (p f)(\psi)(x) + (q g)(\psi)(x).
  $$
- Подставим условия:
  $$
  (p(\psi))(f(\psi))(x) + (q(\psi))(g(\psi))(x) = (p(\psi))(0) + (q(\psi))(0) = 0.
  $$
- Следовательно, $x = 0$.

# Сумма корневых пространств — прямая.
## Теорема 12

- Пусть $V$ — линейное пространство над полем $K$, $\varphi \in \text{End}(V)$, $\text{Spec}(\varphi) = \{\lambda_1, \ldots, \lambda_k\}$. Тогда:
  $$
 \sum_{i=1}^kV(\lambda_i)-прямая\ сумма
  $$

### Доказательство

- Пусть $W_i = \sum_{j \neq i} V(\lambda_j)$.
-  По критерию прямой суммы нам достаточно доказать, что $W_i \cap V(λ_i) = \{0\}$.
- Пусть $x ∈ W_i ∩V(λ_i)$. Тогда $(φ−λ_iid)^n(x) = 0$, так как $x ∈V(λ_i)$.
- С другой стороны, $x = \sum_{j \neq i} x_j$, где $x_j \in V(\lambda_j)$.
- Следовательно:
  $$
  (\varphi - \lambda_j \cdot \text{id})^n(x_j) = 0.
  $$

Рассмотрим многочлен $f(t)$
 $$
  f(t) = \prod_{j \neq i} (t - \lambda_j)^n.
  $$
- Тогда:
  $$
  f(\varphi)(x) = \sum_{j \neq i} (f(\varphi))(x_j) = \\ 
  \sum_{j\ne i}\left(\prod(\varphi-\lambda_sid)^n \right)\left((\varphi-\lambda_jid)^n(x_j)\right)=\\
  \sum_{j\ne i}\left(\prod_{s\not\in \{i,j\}}(\varphi-\lambda_sid)^n \right)(0)=0
  $$



- Многочлены $(t - \lambda_i)^n$ и $f(t)$ взаимно просты, так как в разложении $f(t)$ на линейные множители нет $t - \lambda_i$.
- По [Лемме 8](#лемма-8), из $(\varphi - \lambda_i \cdot \text{id})^n(x) = 0$ и $f(\varphi)(x) = 0$ следует, что $x = 0$.

- Таким образом, $W_i \cap V(\lambda_i) = \{0\}$, что завершает доказательство.

# Разложении пространства в прямую сумму корневых. Инвариантность корневых подпространств.
## Теорема 13

- Пусть $V$ — линейное пространство над полем $K$, $\dim(V) = n$, $\varphi \in \text{End}(V)$, $\text{Spec}(\varphi) = \{\lambda_1, \ldots, \lambda_k\}$ и:
  $$
 \chi_\varphi(t) = (-1)^n \prod_{i=1}^k (t - \lambda_i)^{m_i}.
  $$
- Тогда выполнены следующие утверждения:
  1. $V = \bigoplus_{i=1}^k V(\lambda_i)$.
  2. Корневые пространства $\varphi$-инвариантны.

### Доказательство

#### Утверждение 1

- Пусть $f_i(t) = \prod_{j \neq i} (t - \lambda_j)^{m_j}$.
- Понятно, что $(f_1, \ldots, f_k) = 1$ (мы знаем разложение каждого из этих многочленов на линейные множители, и ни один из них не является общим для всех $k$ многочленов, значит, эти многочлены взаимно просты в совокупности).
- Тогда по теореме о линейном представлении НОД существуют такие многочлены $h_1, \ldots, h_k \in K[t]$, что:
  $$
  h_1 f_1 + \cdots + h_k f_k = 1.
  $$
- Подставив в это равенство оператор $\varphi$, мы получим:
  $$
  (h_1 f_1)(\varphi) + \cdots + (h_k f_k)(\varphi) = \text{id}.
  $$
- Для линейного отображения $\psi \in \text{End}(V)$ и $X \subset V$ будем использовать обозначение $\psi(X) = \{\psi(x) : x \in X\}$.
- Пусть $W_i = ((h_i f_i)(\varphi))(V)$.
- Подставив в качестве аргумента в равенство пространство $V$, мы получим:
  $$
  V = \text{id}(V) = \sum_{i=1}^k (h_i f_i)(\varphi)(V) = \\ \sum_{i=1}^k\left((h_if_i)(\varphi)\right)(V)=\sum_{i=1}^kW_i \tag{2}
  $$

#### Утверждение $W_i \subset V(\lambda_i)$

- Пусть $y \in W_i$. Тогда $y = ((h_i f_i)(\varphi))(x)$, где $x \in V$. Следовательно:
  $$
  (\varphi - \lambda_i \cdot \text{id})^{m_i}(y) = (\varphi - \lambda_i \cdot \text{id})^{m_i} \cdot h_i(\varphi) \cdot f_i(\varphi)(x).=\\(h_i(\varphi))(((\varphi-\lambda_iid)^{m_i}\cdot f_i(\varphi))(x))=\\(h_i(\varphi))(\prod_{j=1}^k(\varphi-\lambda_jid)^{m_j}(x))=\\(h_i(\varphi))((-1)^n\cdot (\chi_\varphi(\varphi))(x))=\\(h_i(\varphi))(x)=0
  $$

  так как $\chi_\varphi(\varphi)$ — нулевой оператор по теореме [Гамильтона-Кэли](#теорема-гамильтона-кэли).
- Следовательно, $y \in V(\lambda_i)$.

#### Доказательство утверждения 1 теоремы

- По [Теореме 12](#теорема-12) сумма корневых подпространств оператора $\varphi$ — прямая. Значит:
  $$
  \bigoplus_{i=1}^k V(\lambda_i) < V.
  $$
- С другой стороны, мы знаем, что $V = \bigoplus_{i=1}^k W_i$.
- Но $\bigoplus_{i=1}^k W_i$ — подмножество $\bigoplus_{i=1}^k V(\lambda_i)$, так как $W_i \subset V(\lambda_i)$.
- Это возможно лишь при:
  $$
  V = \bigoplus_{i=1}^k V(\lambda_i).
  $$

#### Утверждение 2

- Так как мы доказали, что $W_i = V(\lambda_i)$, нам достаточно доказать, что $W_i$ — $\varphi$-инвариантно.
- Проверим это. Действительно, пусть $y \in W_i$, тогда $y = ((h_i f_i)(\varphi))(x)$, где $x \in V$.
- Следовательно:
  $$
  \varphi(y) = \varphi((h_i f_i)(\varphi)(x)) = (h_i f_i)(\varphi)(\varphi(x)) \in W_i,
  $$
  так как $\varphi(x) \in V$, а $W_i = ((h_i f_i)(\varphi))(V)$.

# Размерность корневого подпространства.
## Теорема 14

- Пусть $V$ — линейное пространство над полем $K$, $\dim(V) = n$, $\varphi \in \text{End}(V)$, $\text{Spec}(\varphi) = \{\lambda_1, \ldots, \lambda_k\}$ и:
  $$
  \chi_\varphi(t) = (-1)^n \prod_{i=1}^k (t - \lambda_i)^{m_i}.
  $$
- Тогда выполнены следующие утверждения:
  1. Пусть $\varphi_i = \varphi|_{V(\lambda_i)}$. Тогда оператор $\varphi_i \in \text{End}(V(\lambda_i))$ имеет единственное собственное число — $\lambda_i$.
  2. $\dim(V(\lambda_i)) = m_i$.

### Доказательство

#### Утверждение 1

- Подчеркнем, что формулировка пункта 1 корректна, так как $V(\lambda_i)$ — это $\varphi$-инвариантное пространство по Теореме 13.
- Предположим противное, пусть $\mu \in \text{Spec}(\varphi_i)$, $\mu \neq \lambda_i$, а $x$ — собственный вектор $\mu$.
- Тогда:
  $$
  (\varphi - \mu \cdot \text{id})(x) = 0.
  $$
- С другой стороны, так как $x \in V(\lambda_i)$, мы имеем:
  $$
  (\varphi - \lambda_i \cdot \text{id})^n(x) = 0.
  $$
- Очевидно, многочлены $t - \mu$ и $(t - \lambda_i)^n$ взаимно просты, откуда по [Лемме 8](#лемма-8) имеем $x = 0$, что противоречит определению собственного вектора.

#### Утверждение 2

- По Теореме 13:
  $$
  V = \bigoplus_{i=1}^k V(\lambda_i).
  $$
- В каждом пространстве $V(\lambda_i)$ зафиксируем свой базис, пусть $A_i$ — матрица отображения $\varphi_i$ в этом базисе.
- По Лемме 5, тогда в базисе $V$, полученном объединением зафиксированных выше базисов, отображение $\varphi$ имеет матрицу:
  $$
  A = \text{diag}(A_1, \ldots, A_k).
  $$
- Пусть $n_i = \dim(V(\lambda_i))$. Тогда:
  $$
  (-1)^n \prod_{i=1}^k (t - \lambda_i)^{m_i} = \chi_\varphi(t) = \det(A - t E_n) =\\ \det(\text{diag}(A_1,\ldots,A_k)-t\cdot \text{diag}(E_{n_1},\ldots,E_{n_k}))= \\\det(\text{diag}(A_1 - t E_{n_1}, \ldots, A_k - t E_{n_k}) =\\ \prod_{i=1}^k\det(A_i-tE_{n_i})=\prod\chi({\varphi_i}(t) \tag{*}.
  $$
- По пункту 1, $\text{Spec}(\varphi_i) = \{\lambda_i\}$, следовательно:
  $$
  \det(A_i - t E_{n_i}) = \chi_{\varphi_i}(t).
  $$
- Тогда по Теореме 9 многочлен $\chi_{\varphi_i}(t)$ имеет единственный корень $\lambda_i$
- По формуле $(*)$
$$
(-1)^n\prod){i=1}^k(t-\lambda_i)^{m_i}=\prod_{i=1}^k\chi_{\varphi_i}(t)
$$
• Так как для каждого $i ∈ \{1,...,k\}\ χ_{φ_i} (t)$ имеет единственный корень $λ_i$, остается единственная возможность:
  $$
  \chi_{\varphi_i}(t) = (-1)^{m_i}(t - \lambda_i)^{m_i}.
  $$
- Подставим это в формулу:
  $$
  (-1)^n \prod_{i=1}^k (t - \lambda_i)^{m_i} = \prod_{i=1}^k (-1)^{n_i}(t - \lambda_i)^{n_i}.
  $$
- Следовательно, $\dim(V(\lambda_i)) = m_i$.

# Относительный базис.

## Определение

- Пусть $U < V$.
- Вектора $e_1, \ldots, e_s$ из $V$ линейно независимы над $U$, если никакая их нетривиальная линейная комбинация не лежит в $U$.
- Относительный базис $V$ над $U$ — это $\dim(V) - \dim(U)$ линейно независимых над $U$ векторов.

## Свойство 1

- $e_1, \ldots, e_r \in V$ линейно независимы над $U$, если и только если $\overline{e_1}, \ldots, \overline{e_r}$ линейно независимы в факторпространстве $V / U$.

### Доказательство

- Очевидно ввиду того, что равенство нулю линейной комбинации $\overline{e_1}, \ldots, \overline{e_r}$ в $V / U$ равносильно принадлежности $U$ аналогичной линейной комбинации векторов $e_1, \ldots, e_r$ в $V$.

### Свойство 2

- $e_1, \ldots, e_r$ — относительный базис $V$ над $U$, если и только если $\overline{e_1}, \ldots, \overline{e_r}$ — базис $V / U$.

#### Доказательство

- К Свойству 1 нужно лишь добавить, что по доказанному в главе "Линейные пространства" $\dim(V / U) = \dim(V) - \dim(U)$.


### Свойство 3

- Относительный базис $V$ над $U$ — максимальное множество векторов из $V$, линейно независимых над $U$.
- Если $e_1, \ldots, e_s \in V$ линейно независимы над $U$, то эти вектора можно дополнить до относительного базиса $V$ над $U$.

#### Доказательство

- Воспользуемся Свойствами 1 и 2, а также тем, что любое линейно независимое множество в $V / U$ можно дополнить до базиса.

# Разбиение корневого пространства на ядра. Лемма о ЛНЗ векторов над $W_{t−2}$.

## Разбиение корневого пространства на ядра

- Пусть $V$ — линейное пространство над полем $K$, $\dim(V) = n$, $\varphi \in \text{End}(V)$, $\lambda \in \text{Spec}(\varphi)$, а $m$ — кратность корня $\lambda$ в характеристическом многочлене $\chi_\varphi(t)$.
- Тогда $\dim(V(\lambda)) = m$ по Теореме 14.
- Пусть $\psi = \varphi - \lambda \cdot \text{id}$.
- Введем обозначения:
  $$
  W_0 := \{0\}, \quad W_i := \ker(\psi^i) \ \text{для} \ i \in \mathbb{N}.
  $$
- Понятно, что:
  $$
  W_0 \leq W_1 \leq \cdots \leq W_n = V(\lambda),
  $$
  (последнее равенство следует из свойства корневых пространств).
- Пусть $\ell$ — минимальное такое натуральное число, что $W_\ell = V(\lambda)$.
- Так как $W_i \leq V(\lambda)$, мы имеем:
  $$
  V(\lambda) = W_\ell = W_{\ell+1} = \cdots,
  $$
  поэтому все последующие ядра после $W_\ell$ не будут меняться.
- Введем обозначения:
  $$
  p_i := \dim(W_i) \quad \text{(здесь $i \in \{0, \ldots, \ell\}$)},
  $$
  $$
  r_i := p_i - p_{i-1} \quad \text{(здесь $i \in \{1, \ldots, \ell\}$)}.
  $$
  • Отметим, что количество векторов в относительном базисе $W_ℓ$ по $W_{t−1}$ равно
  $$
  \dim(W_\ell)-\dim(W_{t-1})=\sum_{i=1}^l(\dim(W_i)-\dim(W_{i-1}))=\sum_{i=t}^jr_i
  $$
  
  ## Лемма 9
- Пусть $2 \leq t \leq \ell$ и у нас есть таблица, строки которой занумерованы числами от $t$ до $\ell$. 
- В строке с номером $s$ стоят вектора $e^s_1, \ldots, e^s_{r_s} \in W_s$. 
- Предположим, что для каждого $s \in \{t, \ldots, \ell\}$ вектора в $s$-й строке линейно независимы над $W_{s-1}$. 
- Тогда все записанные в таблице вектора, а также $\psi(e^t_1), \ldots, \psi(e^t_{r_t})$ вместе линейно независимы над $W_{t-2}$. 
- В частности, эти вектора можно дополнить до относительного базиса $W_\ell$ над $W_{t-2}$.

#### Доказательство

- Предположим противное. Пусть:
  $$
  \sum_{s=t}^\ell \sum_{i=1}^{r_s} \alpha^s_i e^s_i + \sum_{j=1}^{r_t} \beta_j \psi(e_j^t) = w, \quad w \in W_{t-2}, \tag{*}
  $$
  где не все коэффициенты $\alpha_i^s$ и $\beta_j$ равны $0$. 

- Разберем два случая:
  1. Не все коэффициенты $\alpha_i^s$ равны $0$.
  2. Все коэффициенты $\alpha_i^s$ равны $0$.

## Случай 1: не все $\alpha_s^i$ равны $0$

- Пусть $q$ — наибольшее такое число, что существует отличный от $0$ индекс $\alpha_i^q$.
- Тогда $\alpha_i^s = 0$ при $s > q$, и в первой сумме из $(*)$ можно вести суммирование до $q$ вместо $\ell$.
- Подействуем на обе части $(*)$ оператором $\psi^{q-1}$ и получим:
  $$
  0=\psi^{q-1}(w)=\sum_{j=1}^{r_t} \beta_j \psi^{q}(e_j^t) + \sum_{s=t}^{q} \sum_{i=1}^{r_s} \alpha_i^s \psi^{q-1}(e_i^s)\\=\sum_{i=1}^{r_q}\alpha_i^{q}\psi^{q-1}(e_i^q) = \psi^{q-1}(\sum_{i=1}^{r_q}\alpha_i^qe^q_i),
  $$
  откуда следует, что:
- Следовательно, $\sum_{i=1}^{r_q} \alpha_i^q e_i^q \in W_{q-1}$, что противоречит условию.

## Случай 2: все $\alpha_i^s$ равны $0$

- Тогда существует $\beta_j \ne 0$, а вся первая сумма из $(*)$ — нулевая, и $(*)$ превращается в:
  $$
  w = \sum_{j=1}^{r_t} \beta_j \psi(e_j^t) = \psi\left(\sum_{j=1}^{r_t} \beta_j e_j^t\right).
  $$
- Тогда:
  $$
  0 = \psi^{t-2}(w) = \psi^{t-1}\left(\sum_{j=1}^{r_t} \beta_j e_j^t\right).
  $$
- Следовательно:
  $$
  \sum_{j=1}^{r_t} \beta_j e_j^t \in W_{t-1},
  $$
  что противоречит условию.

### Следствие 3

- Для всех $t \in \{2, \ldots, \ell\}$ выполнено $r_{t-1} \geq r_t$.

#### Доказательство

- Из Леммы 9 следует, что:
  $$
  r_t + \sum_{j=t}^\ell r_j \leq \dim(W_\ell) - \dim(W_{t-2}) = \sum_{j=t-1}^\ell r_j
  $$
  откуда следует, что $r_{t-1} \geq r_t$.

# Лемма о дополнении до относительного базиса.
### Лемма 10

- Пусть $2 \leq t \leq \ell$ и у нас есть таблица, строки которой занумерованы числами от $t$ до $\ell$.
- В строке с номером $i$ стоят вектора $e_1^i, \ldots, e^i_{r_i} \in W_i$.
- Предположим, что для каждого $i \in \{t, \ldots, \ell\}$ вектора в строках с $i$ по $\ell$ образуют относительный базис $W_\ell$ над $W_{i-1}$.
- Пусть $T$ — множество, состоящее из всех векторов в таблице, а также $\psi(e_1^t), \ldots, \psi(e^t_{r_t})$.
- Тогда вектора из $T$ можно дополнить до относительного базиса $W_\ell$ над $W_{t-2}$, дописав $r_{t-1} - r_t$ векторов из $W_{t-1}$.

#### Доказательство

- Пусть $s = r_{t-1} - r_t$. По Следствию 3, $s \geq 0$.
- По Лемме 9, вектора из $T$ линейно независимы над $W_{t-2}$, и их можно дополнить до относительного базиса $W_\ell$ над $W_{t-2}$ векторами $x_1, \ldots, x_s \in W_\ell$. (Нужно в точности $s$ векторов, как видно из вычислений Следствия 3.)
- Так как $e_1^t, \ldots, e^t_{r_t}, \ldots, e^\ell_1, \ldots, e^\ell_{r_\ell}$ — относительный базис $W_\ell$ по $W_{t-1}$.
- Поэтому в факторпространстве $W_\ell / W_{t-1}$ для любого $q \in \{1, \ldots, s\}$ мы имеем:
  $$
  \overline{x_q} = \sum_{j=t}^\ell \sum_{i=1}^{r_j} \beta_{i,q}^j e_i^j,
  $$
  где все $\beta_{i,q}^j \in K$.
  ### Положим

- Пусть $y_q = x_q - \sum_{j=t}^\ell \sum_{i=1}^{r_j} \beta^j_{i,q} e_i^j$. $(\ast)$
- Тогда $\overline{y_q} = 0$ в $W_\ell / W_{t-1}$, значит, $y_q \in W_{t-1}$.
- Докажем, что $y_1, \ldots, y_s \in W_{t-1}$ также дополняют вектора из $T$ до относительного базиса $W_\ell$ над $W_{t-2}$.
- Для этого достаточно показать, что все эти вектора линейно независимы (ЛНЗ) над $W_{t-2}$. Пусть это не так, и
  $$
  \sum_{i=1}^s \gamma_i y_i + \sum_{j=t}^\ell \sum_{i=1}^{r_j} \delta_i^j e_i^j + \sum_{i=1}^{r_t} \alpha_i \psi(e_i^t) = w \in W_{t-2},
  $$
  где все $\gamma_i, \alpha_i, \delta_i^j \in K$ и не все они равны $0$.
- Из [Леммы 9](#лемма-9) следует, что не все $\gamma_1, \ldots, \gamma_s$ равны $0$.
- Подставив для каждого $y_i$ выражение $(\ast)$, после приведения подобных членов, получим:
  $$
  \sum_{i=1}^s \gamma_i x_i + \sum_{j=t}^\ell \sum_{i=1}^{r_j} \varepsilon_i^j e_i^j + \sum_{i=1}^{r_t} \alpha_i \psi(e_i^t) = w \in W_{t-2},
  $$
  где $\varepsilon_i^j = \delta_i^j + \sum_{q=1}^s \gamma_q \beta^j_{i,q}$.
- Но по выбору $x_1, \ldots, x_s$, тогда $\gamma_1 = \cdots = \gamma_s = 0$, что является противоречием.